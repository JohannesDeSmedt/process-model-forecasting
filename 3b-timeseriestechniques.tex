\subsection{Time series techniques}\label{sec:3b:timeseries}

To model the time series of DFs, various algorithms can be used.
In time series modelling, the main objective is to obtain a forecast or prediction $\hat{y}_{T+h|T}$ for a horizon $h\in \mathbb{N}$ based on previous $T$ values in the series $(y_1,...,y_T)$ \cite{hyndman2018forecasting}.
For example, the naive forecast simply uses the last value of the time series $T$ as its prediction $\hat{y}_{T+h|T}=y_T$.
An alternative naive forecast uses the average value of the time series $T$ as its prediction $\hat{y}_{T+h|T}=\frac{1}{T}\Sigma_i^{T} y_i$.
A wide array of such forecasting techniques exist, ranging from simple models such as naive forecasts over to more advanced approaches such as exponential smoothing and auto-regressive models.
Many also exist in a seasonal variant due to their application in contexts such as sales forecasting.
We briefly discuss smoothing models, autoregressive, moving averages and ARIMA models, and varying variance models which make up the main families of traditional time series forecasting \cite{hyndman2018forecasting}.

A Simple Exponential Smoothing (SES) model uses a weighted average of past values where their importance exponentially decays as they are further into the past where Holt's models introduce a trend in the forecast, meaning the forecast is not flat.
Exponential smoothing models often perform very well despite their simple setup \cite{makridakis2018statistical}.

AutoRegressive Integrating Moving Average (ARIMA) models are based on auto-correlations within time series. 
They combine auto-regressions with a moving average over error terms.
It is established by a combination of an AutoRegressive (AR) model of order $p$ uses the past $p$ values in the time series and applies a regression over them and a Moving Average (MA) model of order $q$ which creates a moving average of the past forecast errors.
Given the necessity of using a white noise series for AR and MA models, data is often differenced to obtain such series.
ARIMA models then combine both AR and MA models where the integration is taking place after modelling as these models are fitted over differenced time series.
ARIMA models are considered to be one of the strongest time series modelling techniques.

An extension to ARIMA which is widely used in econometrics exists in (Generalized) AutoRegressive Conditional Heteroskedasticity ((G)ARCH) models \cite{francq2019garch}.
They resolve the assumption that the variance of the error term has to be equal over time, but rather model this variance as a function of the previous error term.
For AR-models, this leads to the use of ARCH-models, while for ARMA models GARCH-models are used as follows.
An ARCH(q) model captures the change in variance by allowing it to both gradually increase over time, or to allow for short bursts of increased variance.
A GARCH(p,q) model combines both the past values of observations as well as the past values of variance.
(G)ARCH models often outperform ARIMA models in contexts such as the prediction of financial indicators of which the variance often changes over time \cite{francq2019garch}.
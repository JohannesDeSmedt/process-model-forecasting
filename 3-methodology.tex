\section{Methodology}\label{sec:methodology}
This section outlines the forecasting techniques that will be used, as well as their connection with time series extracted from event logs.

\subsection{Time series techniques}
To model the time series of DFGs, various algorithms are used.
In time series modelling, the main objective is to obtain a forecast or prediction $\hat{y}_{T+h|T}$ for a horizon $h\in \mathbb{N}$ based on previous $T$ values in the series $(y_1,...,y_T)$ \cite{hyndman2018forecasting}.
A wide array of time series techniques exist, ranging from simple models such as naive forecasts over to more advanced approaches such as exponential smoothing and auto-regressive models.
Many also exist in a seasonal variant due to their application in contexts such as sales forecasting.
Below, the most-widely used techniques are formalised.

The naive forecast simply uses the last value of the time series $T$ as its prediction:
\[\hat{y}_{T+h|T}=y_T\]
An alternative naive forecast uses the average value of the time series $T$ as its prediction:
\[\hat{y}_{T+h|T}=\frac{1}{T}\Sigma_i^{T} y_i\]

A Simple Exponential Smoothing (SES) model uses a weighted average of past values where their importance exponentially decays as they are further into the past:
\[\hat{y}_{T+h|T}=\alpha y_T + (1-\alpha)\hat{y}_{T|T-1}\]
with $\alpha \in [0,1]$ a smoothing parameter where lower values of $\alpha$ allow for focusing on the more recent values more.
Holt's models introduce a trend in the forecast, meaning the forecast is not flat:
\[\hat{y}_{T+h|T}=l_t + hb_t\]
with $l_t = \alpha y_t + (1-\alpha)(l_{t-1}+b_{t-1})$ modelling the overall level of the time series and $b_t=\beta(l_t-l_{t-1})+(1-\beta)b_{t-1}$ modelling the trend over the series where $\alpha$ is as before and $\beta$ the smoothing parameter for the trend.
Exponential smoothing models often perform very well despite their simple setup.

AutoRegressive Integrating Moving Average (ARIMA) models are based on auto-correlations within time series. 
They combine auto-regressions with a moving average over error terms.

An AutoRegressive (AR) model of order $p$ uses the past $p$ values in the time series and applies a regression over them. 
It can be written as:
\[y_t=c+\phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + \epsilon_t\]
where all $\phi_i,\, i\in[1,p]$ can assign different weights to different time lags.

A Moving Average (MA) model of order $q$ can be written as:
\[y_t=c+\epsilon_t + \phi_1\epsilon_{t-1}+\dots + \phi_q\epsilon_{t-q}\]
with $\phi>0$ a smoothing parameter, and $\epsilon_t$ again a white noise series, hence the model creates a moving average of the past forecast errors.
Given the necessity of using a white noise series for AR and MA models, data is often differenced to obtain such series.

ARIMA models then combine both AR and MA models where the integration is taking place after modelling as these models are fitted over differenced time series. 
An ARIMA($p,d,q$) model can be written as:
\[y'_t=c + \phi_1 y'_{t-1} + \dots + \phi_p y'_{t-p} + \theta_1 \epsilon_{t-1} + \dots + \theta_q \epsilon_t\]
with $y'_t$ a differenced series of order $d$.
Forecasting is possible by introducing $T+h$ to the equation and iteratively obtaining results by starting off with $T+1$.
ARIMA models are considered to be one of the strongest time series modelling techniques.

An extension to ARIMA which is widely used in econometrics exists in (Generalized) AutoRegressive Conditional Heteroskedasticity ((G)ARCH) models \cite{francq2019garch}.
They resolve the assumption that the variance of the error term has to be equal over time, but rather model this variance as a function of the previous error term.
For AR-models, this leads to the use of ARCH-models, while for ARMA models GARCH-models are used as follows.

An ARCH(q) model captures the change in variance over time as follows:
\[\sigma^2_t = \alpha_0 + \alpha_1 y^2_{t-1} + \dots + \alpha_q y^2_{t-q}\] is the variance over time
with $\alpha_0 > 0$ and $\alpha_i\geq 0,\,i\in[1,q]$ smoothing parameters, and $y_t=\sigma_t\epsilon_t$ where $\epsilon_t \overset{i.i.d}{\sim} (\mu=0, \sigma^2=1)$.
This can be used to capture that the variance  gradually increases over time, or has short bursts of increased variance.

A GARCH(p,q) model combines both the past values of observations as well as the past values of variance:
\[\sigma^2_t = \alpha_0 + \alpha_1 y^2_{t-1} + \dots + \alpha_q y^2_{t-q} + \dots + \beta_1\sigma^2_{t-1} + \dots + \beta_p\sigma^2_{t-p}\]
with $\alpha_0 > 0$ and $\alpha_i,\beta_i\geq 0,\,i>0$.

(G)ARCH models often outperform ARIMA models in contexts such as the prediction of financial indicators of which the variance often changes over time \cite{francq2019garch}.


\subsection{Forecasting Directly Follows series}
A trade-off exists between approaching DFGs as a multivariate collection of DF time series, or treating each DF separately.
The aforementioned time series techniques all use univariate data in contrast with, e.g., Vector AutoRegression (VAR) models, or machine learning-based methods such as neural networks or random forest regressors.
Despite their simple setup, it is debated whether traditional statistical approaches are necessarily outperformed by machine learning methods. 
\cite{makridakis2018statistical} found that this is not the case on a large number of datasets and note that the machine learning algorithms require significantly more computational power, a result that was later reaffirmed although it is noted that hybrid solutions are effective \cite{makridakis2020m4}.
Especially for longer horizons traditional time series approaches still outperform machine learning-based models.
Given the potentially high number of DF pairs in a process' DFG, the proposed approach uses a time series algorithm for each DF series separately.
VAR models would require a high number of intervals (at least as many as there are DFs times the lag coefficient) to be able to estimate all parameters of such a high number of time series despite their potentially strong performance \cite{thomakos2004naive}.
Machine learning models could potentially leverage interrelations between the different DFs but again would require long training times to account for dimensionality issues due to the potentially high number of DFs. Therefore, in this paper, traditional time series approaches are chosen and applied to the univariate DF time series where these have at least 1 observation per sublog/time step present.